# 第三阶段：动作选择机制设计

## 1. 目标

在保证训练鲁棒性的前提下，通过引入**动作池（Action Pool）**机制，提升 SAT 求解器在评估阶段的运行效率。

## 2.核心思想

在推理（评估）过程中，主要的性能瓶颈是在每一步决策时都需要重复地进行神经网络的前向传播。我们的核心思想是通过一次计算、多次使用的方式，来分摊单次前向传播的成本。

- **训练模式**：智能体必须基于最准确、最新的 Q 值进行决策，以保证学习的有效性。因此，我们将继续使用标准的 **ε-Greedy (ε-贪心)** 策略，即在每一步都重新计算 Q 值。

- **评估模式**：此模式下的主要目标是衡量求解器的性能和速度。我们可以用 Q 值的少量“过时”来换取决策速度的显著提升。因此，动作池机制将**仅在此模式下启用**。

## 3. 实现细节

相关逻辑将在 `gqsat/agents.py` 文件中的 `GraphAgent` 类的 `act` 方法中实现。

### 3.1. 区分不同模式

我们将利用 PyTorch 的标准约定：`self.net.training`。这个布尔类型的标志位在模型处于训练模式时（由 `model.train()` 设置）为 `True`，在处于评估模式时（由 `model.eval()` 设置）为 `False`。

### 3.2. 评估模式逻辑（启用动作池）

当 `not self.net.training` 为 `True` 时：

1.  **检查并验证动作**：如果 `action_pool`（一个 `deque` 双端队列）不为空，循环地从池中弹出一个动作进行验证。只有当该动作对应一个当前仍未赋值的变量时，才返回该动作。这可以防止因图状态变化而选择一个失效的动作。
2.  **填充动作池**：如果 `action_pool` 为空：
    a.  执行一次神经网络的前向传播，计算当前所有未赋值变量的 Q 值。
    b.  找出 Q 值最高的 `K` 个动作，其中 `K` 是 `action_pool_size`（动作池大小）。
    c.  将这 `K` 个最优动作存入 `action_pool`。
    d.  从刚刚填充好的动作池中弹出第一个动作并返回。

### 3.3. 训练模式逻辑 (ε-Greedy)

当 `self.net.training` 为 `True` 时：

1.  生成一个随机数。如果该数小于 `epsilon` (ε)，则随机选择一个有效的动作（一个未赋值的变量 + 一个随机的极性）。这能鼓励模型进行探索。
2.  否则，执行一次前向传播来计算 Q 值，并贪心地选择 Q 值最高的动作。这能利用模型已学到的策略。

## 4. 相关配置

动作池将由一个新的参数 `action_pool_size` 控制。如果此参数被设置为 `0` 或未提供，则动作池机制将在所有模式下被禁用。
