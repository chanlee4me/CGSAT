# 修改图顶点特征定义的思考过程

用户希望为图神经网络（GNN）中的变量节点和子句节点定义新的特征向量。下面是对每个特征的分析和实现思路。

## 1. Horn 子句的定义与识别

Horn 子句的定义是：最多一个文字为正，所有其他文字均为负。
为了计算与 Horn 子句相关的特征，首先需要一个辅助函数 `is_horn_clause(clause_literals)`。
- 输入：一个子句的文字列表（例如，`[1, -2, -3]` 表示 `x1 OR NOT x2 OR NOT x3`）。
- 输出：布尔值（True 如果是 Horn 子句，False 否则）。
- 实现：计算子句中正文字（大于0的整数）的数量。如果数量小于或等于1，则该子句是 Horn 子句。

## 2. 变量节点特征 (5 个)

假设我们有一个 CNF 公式，其中包含一组变量和一组子句。

1.  **`pos_lit_degree`**: 变量的正文字出现的频率。
    *   计算方法：对于给定的变量 `v`，遍历所有子句。统计变量 `v` 在多少个子句中以正文字（即 `v` 本身）的形式出现。

2.  **`neg_lit_degree`**: 变量的负文字出现的频率。
    *   计算方法：对于给定的变量 `v`，遍历所有子句。统计变量 `v` 在多少个子句中以负文字（即 `-v`）的形式出现。

3.  **`lit_pos_neg_ratio`**: (某变量的正文字出现次数) / (负文字出现次数 + 1)。
    *   计算方法：使用上面计算得到的 `pos_lit_degree` 和 `neg_lit_degree`。公式为 `pos_lit_degree / (neg_lit_degree + 1)`，加1是为了避免除以零。

4.  **`horn_occurrence`**: 变量出现在 Horn 子句的次数 / 总子句数。
    *   计算方法：
        *   首先，使用 `is_horn_clause` 函数识别出所有的 Horn 子句。
        *   对于给定的变量 `v`，遍历所有 Horn 子句。统计变量 `v` (无论是正文字还是负文字) 出现在这些 Horn 子句中的次数。
        *   将此计数值除以 CNF 公式中的总子句数。

5.  **`clause_size_sum_inv`**: 所有含该变量的子句长度倒数之和。
    *   计算方法：对于给定的变量 `v`，找到所有包含该变量（无论是正文字还是负文字）的子句。
    *   对于每个这样的子句，获取其长度（即文字的个数）。
    *   计算该长度的倒数。
    *   将所有这些倒数加起来。

## 3. 子句节点特征 (5 个标量特征 + 1 个 10 维向量特征)

1.  **`clause_degree`**: 子句长度 / 变量总数。
    *   计算方法：获取当前子句的长度（文字数量）。获取 CNF 公式中的总变量数。两者相除。

2.  **`is_binary`**: 二元子句标志 (0/1)。
    *   计算方法：如果子句长度等于2，则为1，否则为0。

3.  **`is_ternary`**: 三元子句标志 (0/1)。
    *   计算方法：如果子句长度等于3，则为1，否则为0。

4.  **`is_horn`**: Horn 子句标志 (0/1)。
    *   计算方法：使用前面定义的 `is_horn_clause(clause_literals)` 函数判断当前子句是否为 Horn 子句。是则为1，否则为0。

5.  **`clause_pos_neg_ratio`**: 子句内部正／负字面比例。
    *   计算方法：
        *   统计子句中正文字的数量 (`num_pos`)。
        *   统计子句中负文字的数量 (`num_neg`)。
        *   计算比例 `num_pos / (num_neg + 1)`。

6.  **`clause_pe[0..9]`**: 10 维位置编码，为 GNN 提供字面在子句内的相对顺序信息。
    *   这是一个10维的向量特征。其目的是捕捉子句内部文字的顺序信息。
    *   实现思路：
        *   可以定义一个最大子句长度 `MAX_CLAUSE_LENGTH`（例如，根据数据集的统计特性设定）。
        *   使用一个嵌入层（如 PyTorch 中的 `torch.nn.Embedding(MAX_CLAUSE_LENGTH, 10)`）。这个嵌入层将为每个可能的位置索引（0 到 `MAX_CLAUSE_LENGTH - 1`）学习一个10维的向量。
        *   对于一个具体的子句，其长度为 `L`。子句中的文字位置为 `0, 1, ..., L-1`。
        *   获取这些位置对应的10维嵌入向量。
        *   对这些向量进行聚合（例如，平均池化）得到一个单一的10维向量作为该子句的 `clause_pe` 特征。
        *   如果子句长度超过 `MAX_CLAUSE_LENGTH`，需要进行截断或采用其他策略。如果长度不足，平均池化仍然有效。

## 4. 代码修改点和实现策略

1.  **数据表示**：
    *   需要一种方式来表示 CNF 公式，包括变量、文字和子句。通常，在加载 DIMACS 文件后，会有一个内部的数据结构来存储这些信息。
    *   图的节点（变量节点和子句节点）也需要被明确表示。

2.  **特征提取模块**：
    *   最主要的修改将发生在特征提取部分。这可能是一个或多个函数，负责在构建图（或加载数据）时为每个节点计算其特征向量。
    *   根据项目结构，这些函数可能位于 `gqsat/utils.py` 或与图构建相关的模块中。

3.  **`is_horn_clause` 函数**：
    *   这个函数应该很容易实现，并且可以在特征提取模块中被调用。

4.  **特征计算**：
    *   **变量特征**：需要遍历子句，并可能需要预先计算一些统计信息（如每个变量在哪些子句中出现，以什么形式出现）。
    *   **子句特征**：相对直接，大多基于子句本身的属性（长度、文字构成）。`clause_pe` 的实现需要引入嵌入层和池化操作。

5.  **GNN 模型输入维度调整**：
    *   一旦新的特征被计算出来，GNN 模型的输入层维度需要相应更新。
    *   变量节点的特征维度将是 5。
    *   子句节点的特征维度将是 5 (标量特征) + 10 (位置编码) = 15。
    *   这通常涉及到修改 `gqsat/models.py` 中 GNN 模型的定义，特别是其第一层（例如，`nn.Linear` 或特定 GNN 层的输入维度参数）。

## 5. 潜在挑战与考虑

*   **CNF 数据结构**：确保有一个高效的方式来访问和查询 CNF 公式的结构（例如，快速找到包含某个变量的所有子句，快速获取子句的文字列表等）。
*   **`clause_pe` 的具体实现**：位置编码的有效性可能取决于具体的实现方式（嵌入层的大小、池化方法等）。需要实验来确定最佳配置。
*   **计算效率**：对于非常大的 CNF 公式，特征提取的效率可能需要考虑。一些统计数据（如文字度数）可以预先计算和存储。

## 6. 最终实现思路

综合前面的讨论，我确定以下实现思路：

### 6.1 特征提取模块设计

基于静态特征计算的原则，我设计了`CNFFeatureExtractor`类，它接收原始CNF公式（子句列表和变量总数），并计算所有所需的节点特征。

1. **预处理与缓存**:
   - 初始化时预计算常用统计量（如正/负文字出现次数，Horn子句标识等）
   - 特征计算时充分利用这些缓存数据以提高效率

2. **特征计算方法**:
   - `extract_var_features`: 计算所有变量节点的5维特征
   - `extract_clause_features`: 计算所有子句节点的15维特征（5个标量 + 10维位置编码）

3. **辅助函数**:
   - `is_horn_clause`: 判断子句是否为Horn子句
   - 其他内部统计函数

### 6.2 与MiniSATEnv的集成

修改`MiniSATEnv.py`中的`parse_state_as_graph`方法，在图构建过程中利用特征提取器:

1. **顶点特征维度调整**:
   - 将`vertex_in_size`从2增加到16，以容纳最大的特征向量(子句特征15维 + 1维类型标识符)
   - 变量节点使用0-5维，子句节点使用0-15维

2. **特征计算时机**:
   - 在每次重置环境后的图构建阶段计算特征
   - 特征计算基于原始CNF公式，而不考虑求解过程中的状态变化

3. **Dummy状态处理**:
   - 修改`get_dummy_state`方法，使用适当的默认值填充特征向量

这种设计保持了特征计算的模块化，同时确保了与原有代码的无缝集成。所有特征都在图初始化阶段一次性计算，不会引入额外的运行时开销。

### 6.3 位置编码的实现

对于子句的10维位置编码(`clause_pe`)，我采用了基于`torch.nn.Embedding`的方案：

1. 为每个文字位置生成一个嵌入向量
2. 对于每个子句，取其中所有位置的嵌入向量
3. 使用平均池化方法汇总这些嵌入向量，得到最终的10维位置编码

这种方法能够有效捕捉子句内部文字的相对位置信息，为GNN提供额外的结构线索。

### 6.4 值得注意的实现细节

1. **遍历优化**:
   - 多次使用预计算的统计数据，避免重复遍历
   
2. **数值稳定性**:
   - 在除法操作中添加小的偏移量(+1)以避免除零错误
   
3. **内存效率**:
   - 所有特征计算一次性完成，不需要在求解过程中重复计算
   
4. **类型一致性**:
   - 确保所有特征都转换为np.float32类型，与原有代码保持一致

这种实现方案既符合用户的需求，又能有效地集成到现有系统中，为GNN提供更丰富的节点特征表示，有望提高模型在SAT问题上的性能。

## 7. 关于Q值和变量正负区分的补充思考

在实现过程中，我发现需要考虑以下两个重要方面：

### 7.1 Q值在变量节点中的表示

**观察发现**：
- 当前系统中，变量节点的Q值是由GNN模型(EncoderCoreDecoder)动态计算的，而不是存储在节点特征中
- 模型对每个变量节点输出两个值，对应赋值为true和false的Q值，这体现在模型初始化时的`out_dims=(2, None, None)`参数中

**设计调整**：
- 虽然Q值是动态计算的，但为了概念上的清晰，我们在变量节点特征中预留了2个位置用于表示Q值
- 这样，变量节点的完整特征维度变为：1(标识符) + 5(CNF特征) + 2(Q值预留位) = 8维
- 预留的Q值位置不会直接参与计算，只是为了明确特征空间的设计

### 7.2 变量正负性的区分方式

**当前实现**：
- 系统已经使用边特征来区分变量的正负性，这是一个有效且合理的设计
- 在图构建过程中，边特征设置如下：
  - 如果是正文字，边特征为[0,1]
  - 如果是负文字，边特征为[1,0]
- 这种设计使得GNN可以通过边特征来区分正负文字，而不需要在变量节点本身表示这一信息

**优势分析**：
- 使用边特征区分正负更加灵活，因为同一个变量可能在不同子句中以正或负的形式出现
- 这种设计尊重了SAT问题的本质：变量本身没有正负属性，只有在作为文字出现在子句中时才有正负之分
- GNN的消息传递机制可以有效利用这种边特征来传递文字的正负信息

经过分析，当前代码已经很好地实现了这两个方面。本次修改中，我主要是在特征提取器中为Q值预留了额外的特征位置，并更新了相关的注释和维度说明，使得代码设计意图更加明确。
